// This file was generated by gpufort

#include "hip/hip_complex.h"
#include "hip/hip_runtime.h"
#include "hip/math_functions.h"
#include <algorithm>
#include <cstdio>
#include <iostream>

#include "hipcub/hipcub.hpp"
#include <limits>

#define HIP_CHECK(condition)                                                                                                               \
  {                                                                                                                                        \
    hipError_t error = condition;                                                                                                          \
    if (error != hipSuccess) {                                                                                                             \
      std::cout << "HIP error: " << error << " line: " << __LINE__ << std::endl;                                                           \
      exit(error);                                                                                                                         \
    }                                                                                                                                      \
  }

// global thread indices for various dimensions
#define __gidx(idx) (threadIdx.idx + blockIdx.idx * blockDim.idx)
#define __gidx1 __gidx(x)
#define __gidx2 (__gidx(x) + gridDim.x * blockDim.x * __gidx(y))
#define __gidx3 (__gidx(x) + gridDim.x * blockDim.x * __gidx(y) + gridDim.x * blockDim.x * gridDim.y * blockDim.y * __gidx(z))
#define __total_threads(grid, block) ((grid).x * (grid).y * (grid).z * (block).x * (block).y * (block).z)

namespace {
template <typename I, typename E, typename S> __device__ __forceinline__ bool loop_cond(I idx, E end, S stride) {
  return (stride > 0) ? (idx <= end) : (-idx <= -end);
}

// make float
__device__ __forceinline__ float make_float(const short int &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const unsigned short int &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const unsigned int &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const int &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const long int &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const unsigned long int &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const long long int &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const unsigned long long int &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const signed char &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const unsigned char &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const float &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const double &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const long double &a) { return static_cast<float>(a); }
__device__ __forceinline__ float make_float(const hipFloatComplex &a) { return static_cast<float>(a.x); }
__device__ __forceinline__ float make_float(const hipDoubleComplex &a) { return static_cast<float>(a.x); }
// make double
__device__ __forceinline__ double make_double(const short int &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const unsigned short int &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const unsigned int &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const int &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const long int &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const unsigned long int &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const long long int &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const unsigned long long int &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const signed char &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const unsigned char &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const float &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const double &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const long double &a) { return static_cast<double>(a); }
__device__ __forceinline__ double make_double(const hipFloatComplex &a) { return static_cast<double>(a.x); }
__device__ __forceinline__ double make_double(const hipDoubleComplex &a) { return static_cast<double>(a.x); }
// conjugate complex type
__device__ __forceinline__ hipFloatComplex conj(const hipFloatComplex &c) { return hipConjf(c); }
__device__ __forceinline__ hipDoubleComplex conj(const hipDoubleComplex &z) { return hipConj(z); }

// TODO Add the following functions:
// - sign(x,y) = sign(y) * |x| - sign transfer function
// ...
} // namespace

// reductions
namespace {
struct reduce_op_mult {
  template <typename T> static __host__ __device__ __forceinline__ T ival() { return (T)1; }
  template <typename T> static __host__ __device__ __forceinline__ void init(T &a) { a = ival<T>(); }
  template <typename T> __device__ __forceinline__ T operator()(const T &a, const T &b) const { return a * b; }
};

struct reduce_op_add {
  template <typename T> static __host__ __device__ __forceinline__ T ival() { return (T)0; }
  template <typename T> static __host__ __device__ __forceinline__ void init(T &a) { a = ival<T>(); }
  template <typename T> __device__ __forceinline__ T operator()(const T &a, const T &b) const { return a + b; }
};

struct reduce_op_max {
  template <typename T> static __host__ __device__ __forceinline__ T ival() {
    return -std::numeric_limits<T>::max(); // has negative sign
  }
  template <typename T> static __host__ __device__ __forceinline__ void init(T &a) { a = ival<T>(); }
  template <typename T> __device__ __forceinline__ T operator()(const T &a, const T &b) const { return std::max(a, b); }
};

struct reduce_op_min {
  template <typename T> static __host__ __device__ __forceinline__ T ival() { return std::numeric_limits<T>::max(); }
  template <typename T> static __host__ __device__ __forceinline__ void init(T &a) { a = ival<T>(); }
  template <typename T> __device__ __forceinline__ T operator()(const T &a, const T &b) const { return std::min(a, b); }
};

template <typename T, typename ReduceOpT> void reduce(const T *const d_in, const int &NX, const T *h_out) {
  T *d_out = nullptr;
  hipMalloc((void **)&d_out, sizeof(T));
  // Determine temporary device storage requirements
  void *temp_storage = nullptr;
  size_t temp_storage_bytes = 0;
  ReduceOpT reduceOp;
  hipcub::DeviceReduce::Reduce(temp_storage, temp_storage_bytes, d_in, d_out, NX, ReduceOpT(), ReduceOpT::template ival<T>());
  // Allocate temporary storage
  hipMalloc(&temp_storage, temp_storage_bytes);
  // Run reduction
  hipcub::DeviceReduce::Reduce(temp_storage, temp_storage_bytes, d_in, d_out, NX, ReduceOpT(), ReduceOpT::template ival<T>());
  hipMemcpy((void *)h_out, d_out, sizeof(T), hipMemcpyDeviceToHost);
  // Clean up
  hipFree(d_out);
  hipFree(temp_storage);
}
} // namespace

// end of preamble
#define divideAndRoundUp(x, y) ((x) / (y) + ((x) % (y) != 0))

// BEGIN krnl_cecba2_-1
/* Fortran original:
! parallel loop
  do i = 1, N
     x(i) = 1
     y(i) = 2
  end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): -1-1-1. ('-1' means not specified)
// - Threads per block (CUDA): -1-1-1. ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_cecba2_ - 1(int x, const int x_n1, const int x_lb1, int N, int y, const int y_n1, const int y_lb1) {
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))
#undef _idx_y
#define _idx_y(a) ((a - (y_lb1)))

  int i = 1 + (1) * (threadIdx.x + blockIdx.x * blockDim.x);
  if (loop_cond(i, N, 1)) {
    x[_idx_x(i)] = 1;
    y[_idx_y(i)] = 2;
  }
}

extern "C" void launch_krnl_cecba2_ - 1(dim3 *grid,
                                        dim3 *block,
                                        const int sharedMem,
                                        hipStream_t stream,
                                        int x,
                                        const int x_n1,
                                        const int x_lb1,
                                        int N,
                                        int y,
                                        const int y_n1,
                                        const int y_lb1) {

  // launch kernel
  hipLaunchKernelGGL((krnl_cecba2_ - 1), *grid, *block, sharedMem, stream, x, x_n1, x_lb1, N, y, y_n1, y_lb1);
}
extern "C" void launch_krnl_cecba2_ -
    1_auto(const int sharedMem, hipStream_t stream, int x, const int x_n1, const int x_lb1, int N, int y, const int y_n1, const int y_lb1) {
  const unsigned int krnl_cecba2_ - 1_blockX = 256;
  dim3 block(krnl_cecba2_ - 1_blockX);
  const unsigned int krnl_cecba2_ - 1_NX = (1 + ((N) - (1)));

  const unsigned int krnl_cecba2_ - 1_gridX = divideAndRoundUp(krnl_cecba2_ - 1_NX, krnl_cecba2_ - 1_blockX);
  dim3 grid(krnl_cecba2_ - 1_gridX);

  // launch kernel
  hipLaunchKernelGGL((krnl_cecba2_ - 1), grid, block, sharedMem, stream, x, x_n1, x_lb1, N, y, y_n1, y_lb1);
}
// END krnl_cecba2_-1

// BEGIN krnl_e7eb26_-1
/* Fortran original:
! parallel loop reduction(+:res)
  do i = 1, N
     res = res + x(i)*y(i)
  end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): -1-1-1. ('-1' means not specified)
// - Threads per block (CUDA): -1-1-1. ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_e7eb26_ - 1(int x, const int x_n1, const int x_lb1, int y, const int y_n1, const int y_lb1, int *res, int N) {
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))
#undef _idx_y
#define _idx_y(a) ((a - (y_lb1)))

  int i = 1 + (1) * (threadIdx.x + blockIdx.x * blockDim.x);
  reduce_op_add::init(res[__gidx1]);
  if (loop_cond(i, N, 1)) {
    res[__gidx1] = (res[__gidx1] + x[_idx_x(i)] * y[_idx_y(i)]);
  }
}

extern "C" void launch_krnl_e7eb26_ - 1(dim3 *grid,
                                        dim3 *block,
                                        const int sharedMem,
                                        hipStream_t stream,
                                        int x,
                                        const int x_n1,
                                        const int x_lb1,
                                        int y,
                                        const int y_n1,
                                        const int y_lb1,
                                        int *res,
                                        int N) {

  int *_d_res;
  HIP_CHECK(hipMalloc((void **)&_d_res, __total_threads((*grid), (*block)) * sizeof(int)));

  // launch kernel
  hipLaunchKernelGGL((krnl_e7eb26_ - 1), *grid, *block, sharedMem, stream, x, x_n1, x_lb1, y, y_n1, y_lb1, _d_res, N);
  reduce<int, reduce_op_add>(_d_res, __total_threads((*grid), (*block)), res);
  HIP_CHECK(hipFree(_d_res));
}
extern "C" void launch_krnl_e7eb26_ - 1_auto(const int sharedMem,
                                             hipStream_t stream,
                                             int x,
                                             const int x_n1,
                                             const int x_lb1,
                                             int y,
                                             const int y_n1,
                                             const int y_lb1,
                                             int *res,
                                             int N) {
  const unsigned int krnl_e7eb26_ - 1_blockX = 256;
  dim3 block(krnl_e7eb26_ - 1_blockX);
  const unsigned int krnl_e7eb26_ - 1_NX = (1 + ((N) - (1)));

  const unsigned int krnl_e7eb26_ - 1_gridX = divideAndRoundUp(krnl_e7eb26_ - 1_NX, krnl_e7eb26_ - 1_blockX);
  dim3 grid(krnl_e7eb26_ - 1_gridX);

  int *_d_res;
  HIP_CHECK(hipMalloc((void **)&_d_res, __total_threads((grid), (block)) * sizeof(int)));

  // launch kernel
  hipLaunchKernelGGL((krnl_e7eb26_ - 1), grid, block, sharedMem, stream, x, x_n1, x_lb1, y, y_n1, y_lb1, _d_res, N);
  reduce<int, reduce_op_add>(_d_res, __total_threads((grid), (block)), res);
  HIP_CHECK(hipFree(_d_res));
}
// END krnl_e7eb26_-1
