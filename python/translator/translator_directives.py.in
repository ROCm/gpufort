# SPDX-License-Identifier: MIT                                                
# Copyright (c) 2021 GPUFORT Advanced Micro Devices, Inc. All rights reserved.
# extensions for directive-based programming

def _intrnl_search_values_in_body(ttcontainer,searchFilter,scope,minRank=-1):
    #TODO exclude other annotations as well from this search
    #TODO improve
    def find_all_matching_exclude_directives_(body,filterExpr=lambda x: True,N=-1):
        """
        Find all nodes in tree of type 'searchedType'.
        """
        result = []
        def descend_(curr):
            if (N > 0 and len(result) > N) or isinstance(curr,ILoopAnnotation):
                return
            if filterExpr(curr):
                result.append(curr)
            if isinstance(curr,ParseResults) or\
               isinstance(curr,list):
                for el in curr:
                    descend_(el)
            elif isinstance(curr,TTNode):
                for el in curr.children():
                    descend_(el)
        descend_(ttcontainer.body)
        return result
    
    varnames = [] 
    for ttvalue in find_all_matching_exclude_directives_(ttcontainer.body,searchFilter): # includes the identifiers of the function calls
        name = make_f_str(ttvalue.name())
        name = name.lower()
        ivar,foundInScope = scoper.searchScopeForVariable(\
          scope,create_index_search_tag_for_variable(name))
        if (foundInScope or\
            (not name in KEYWORDS and\
            not name in GPUFORT_CPP_ROUTINES)) and\
            ivar["rank"] >= minRank and\
            not name in varnames: # using set destroys order
                 varnames.append(name)
    return varnames

def _intrnl_variables_in_body(ttcontainer,scope=[]):
    """:return: all identifiers of LValue and RValues in the body."""
    def searchFilter(node):
        return isinstance(node, IValue) and\
               type(node._value) in [TTDerivedTypeMember,TTIdentifier,TTFunctionCallOrTensorAccess]
    result = _intrnl_search_values_in_body(ttcontainer,searchFilter,scope)
    return result
def _intrnl_arrays_in_body(ttcontainer,scope=[]):
    def searchFilter(node):
        return isinstance(node,IValue) and\
                type(node._value) is TTFunctionCallOrTensorAccess 
    return _intrnl_search_values_in_body(ttcontainer,searchFilter,scope,1)
def _intrnl_inout_arrays_in_body(ttcontainer,scope=[]):
    def searchFilter(node):
        return type(node) is TTLValue and\
                type(node._value) is TTFunctionCallOrTensorAccess 
    return _intrnl_search_values_in_body(ttcontainer,searchFilter,scope,1)

def _intrnl_flag_tensors(ttcontainer,scope=[]):
    """Clarify types of function calls / tensor access that are not members of a struct."""
    for expr in find_all(ttcontainer.body,IValue):
        if type(expr._value) is TTFunctionCallOrTensorAccess:
            _, discovered = scoper.searchScopeForVariable(scope,create_index_search_tag_for_variable(expr.name()))
            if discovered:
                expr._value._isTensorAccess = True3

class ILoopAnnotation():
    def numCollapse(self):
        return CLAUSE_NOT_FOUND
    def tileSizes(self):
        return [CLAUSE_NOT_FOUND]
    def gridExpressionFStr(self):
        """ only CUF """
        return ""
    def blockExpressionFStr(self):
        """ only CUF """
        return ""
    def numGangsTeamsBlocks(self):
        return [CLAUSE_NOT_FOUND]
    def numThreadsInBlock(self):
        return [CLAUSE_NOT_FOUND]
    def numWorkers(self): 
        """ only ACC """
        return CLAUSE_NOT_FOUND
    def simdlenVectorLength(self):
        return CLAUSE_NOT_FOUND
    def dataIndependentIterations(self):
        return True
    def privateVars(self,converter=make_f_str): 
        """ CUF,ACC: all scalars are private by default """ 
        return []
    def lastprivateVars(self,converter=make_f_str): 
        """ only OMP """
        return []
    def reductions(self,converter=make_f_str): 
        """ CUF: Scalar lvalues are reduced by default """
        return {}
    def sharedVars(self,converter=make_f_str): 
        """ only OMP """
        return []
    def allArraysAreOnDevice(self):
        """ only True for CUF kernel do directive """
        return False

class TTDoLoop(TTContainer):
    def _assign_fields(self,tokens):
        # Assignment, number | variable
        self.annotation, self._begin, self._end, self._step, self.body = tokens
        if self.annotation == None:
            self.annotation = ILoopAnnotation()
        self._threadIndex = None # "z","y","x"
    def setHipThreadIndex(self,name):
        self._threadIndex = name
    def hipThreadIndexCStr(self):
        indexVar = self.loopVar()
        begin    = make_c_str(self._begin._rhs) # array indexing is corrected in index macro
        step     = make_c_str(self._step)
        return "{indent}int {var} = {begin} + ({step})*(threadIdx.{idx} + blockIdx.{idx} * blockDim.{idx});\n".format(\
                indent=self.indent,var=indexVar,begin=begin,idx=self._threadIndex,step=step)
    def collapsedLoopIndexCStr(self,denominator):
        indexVar = self.loopVar()
        tid      = self._threadIndex
        assert not tid is None
        begin    = make_c_str(self._begin._rhs)
        size     = self.problemSizeCStr()
        step     = make_c_str(self._step)
        # int i<n> = begin<n> + step<n>*(i<denominator<n>> % size<n>)
        return "{indent}int {var} = {begin} + ({step})*({tid}{denom} % {size});\n".format(\
                indent=self.indent,var=indexVar,begin=begin,tid=tid,denom=denominator,size=size,step=step)
    def problemSizeCStr(self):
        if self._step == "1":
            return "(1 + (({end}) - ({begin})))".format(\
                begin=make_c_str(self._begin._rhs),end=make_c_str(self._end),step=make_c_str(self._step) )
        else:
            return "(1 + (({end}) - ({begin}))/({step}))".format(\
                begin=make_c_str(self._begin._rhs),end=make_c_str(self._end),step=make_c_str(self._step))
    def hipThreadBoundCStr(self) :
        indexVar = self.loopVar()
        begin    = make_c_str(self._begin._rhs)
        end      = make_c_str(self._end)
        step     = make_c_str(self._step)
        return "loop_cond({0},{1},{2})".format(indexVar, end, step)
    def loopVar(self,converter=make_c_str):
        return converter(self._begin._lhs)
    def c_str(self):
        indexVar    = self.loopVar()
        begin       = make_c_str(self._begin._rhs) # array indexing is corrected in index macro
        end         = make_c_str(self._end)
        step        = make_c_str(self._step)
        bodyContent = TTContainer.c_str(self)
        if self._threadIndex == None:
            return "{indent}for (int {0}={1}; {0} <= {2}; {0} += {3}) {{\n{4}\n}}".format(self.indent,indexVar, begin, end, step, bodyContent)
        else:
            return bodyContent

class IComputeConstruct():
    def numCollapse(self):
        return CLAUSE_NOT_FOUND
    def numDimensions(self):
        return 1
    def gridExpressionFStr(self):
        """ only CUF """
        return None
    def blockExpressionFStr(self):
        """ only CUF """
        return None
    def numGangsTeamsBlocks(self):
        return [CLAUSE_NOT_FOUND]
    def numThreadsInBlock(self):
        return [CLAUSE_NOT_FOUND]
    def gangTeamPrivateVars(self,converter=make_f_str): 
        return []
    def gangTeamFirstprivateVars(self,converter=make_f_str): 
        return []
    def gangTeamReductions(self,converter=make_f_str): 
        """ CUF,ACC: all scalars are private by default """ 
        return {}
    def variables_in_body(self,scope=[]):
        return []
    def arrays_in_body(self,scope=[]):
        return []
    def inout_arrays_in_body(self,scope=[]):
        return []
    def localScalars(self):
        return []
    def reductionCandidates(self,scope=[]):
        return []
    def loopVars(self):
        return []
    def problemSize(self):
        return []
    def asyncNowait(): 
        """value != CLAUSE_NOT_FOUND means True"""
        return CLAUSE_NOT_FOUND
    def stream(self,converter=make_f_str):
        return "c_null_ptr"
    def sharedMem(self,converter=make_f_str):
        return "0"
    def useDefaultStream(self):
        return True
    def depend(self): 
        """ only OMP """
        #return { "in":[], "out":[], "inout":[], "inout":[], "mutexinoutset":[], "depobj":[] }
        return {}
    def deviceTypes(self): 
        return "*"
    def ifCondition(self): 
        """ OMP,ACC: accelerate only if condition is satisfied. Empty string means condition is satisfied. """
        return ""
    def selfCondition(self): 
        """ OMP,ACC: run on current CPU / device (and do not offload) """
        return ""
    def deviceptrs(self,scope=[]):
        return []
    def createAllocVars(self):
        return []
    def no_createVars(self):
        """ only ACC"""
        return []
    def presentVars(self):
        """ only ACC"""
        return []
    def deleteReleaseVars(self):
        return []
    def copyMapToFromVars(self):
        return []
    def copyinMapToVars(self):
        return []
    def copyoutMapFromVars(self):
        return []
    def attachVars(self):
        """ only ACC """
        return []
    def detachVars(self):
        """ only ACC """
        return []
    def presentByDefault(self):
        """ only ACC parallel """
        return True
    def c_str(self):
        return ""

class TTLoopKernel(TTNode,IComputeConstruct):
    def _assign_fields(self,tokens):
        self._parentDirective, self.body = tokens
        self.scope = scoper.EMPTY_SCOPE
    def __firstLoopAnnotation(self):
        return self.body.annotation
    def __parentDirective(self):
        if self._parentDirective == None:
            return self._firstLoopAnnotation()
        else:
            return self._parentDirective
    def loopVars(self):
        identifierNames = []
        doLoops  = find_all(self.body,TTDoLoop)
        for loop in doLoops:
            identifierNames.append(loop.loopVar(make_f_str))
        return identifierNames
    def variables_in_body(self,scope=[]):
        if len(scope):
            self.scope = scope
        return _intrnl_variables_in_body(self.body,self.scope)
    def arrays_in_body(self,scope=[]):
        if len(scope):
            self.scope = scope
        return _intrnl_arrays_in_body(self.body,self.scope)
    def inout_arrays_in_body(self,scope=[]):
        if len(scope):
            self.scope = scope
        return _intrnl_inout_arrays_in_body(self.body,self.scope)
    def __localScalarsAndReductionCandidates(self,scope):
        """
        local variable      - scalar variable that is not read before the assignment (and is no derived type member)
        reductionCandidates - scalar variable that is written but not read anymore 

        NOTE: Always returns Fortran identifiers
        NOTE: The loop variables need to be removed from this result when rendering the corresponding C kernel.
        NOTE: Implementatin assumes that loop condition variables are not written to in loop body. 
        NOTE: When rendering the kernel, it is best to exclude all variables for which an array declaration has been found,
        from the result list. TTCufKernelDo instances do not know of the type of the variables.
        """
        scalarsReadSoFar   = [] # per line, with name of lhs scalar removed from list
        initializedScalars = [] 

        # depth first search
        assignments = find_all_matching(self.body,
                lambda node: type(node) in [TTAssignment,TTComplexAssignment,TTMatrixAssignment])
        for assignment in assignments:   
            # lhs scalars
            lvalue     = assignment._lhs._value
            lvalueName = make_f_str(lvalue)
            if type(lvalue) is TTIdentifier: # could still be a matrix
                definition,found_in_scope = scoper.searchScopeForVariable(\
                  scope,create_index_search_tag_for_variable(lvalueName))
                if not found_in_scope or definition["rank"] == 0 and\
                        not lvalueName.lower() in scalarsReadSoFar:
                            initializedScalars.append(lvalueName) # read and initialized in 
            # rhs scalars
            rhsIdentifiers = find_all(assignment._rhs,TTIdentifier)
            for ttidentifier in rhsIdentifiers:
                nameLower = ttidentifier.f_str().lower()
                definition,found_in_scope = scoper.searchScopeForVariable(scope,\
                  create_index_search_tag_for_variable(nameLower))
                if (not found_in_scope or definition["rank"] == 0) and\
                        nameLower != lvalueName.lower(): # do not include name of lhs if lhs appears in rhs
                            scalarsReadSoFar.append(nameLower)
        # initialized scalars that are not read (except in same statement) are likely reductions
        # initialized scalars that are read again in other statements are likely local variables
        reductionCandidates = [name for name in initializedScalars if name not in scalarsReadSoFar]
        localScalars        = [name for name in initializedScalars if name not in reductionCandidates] # contains loop variables
        loopVars = [var.lower() for var in self.loopVars()]
        for var in list(localScalars):
            if var.lower() in loopVars:
                localScalars.remove(var)
        return localScalars, reductionCandidates
    def localScalars(self,scope=[]):
        if len(scope):
            self.scope = scope
        localScalars,_ = self.__localScalarsAndReductionCandidates(self.scope)
        return localScalars 
    def reductionCandidates(self,scope=[]):
        if len(scope):
            self.scope = scope
        _,reductionCandidates = self.__localScalarsAndReductionCandidates(self.scope)
        return reductionCandidates
    def problemSize(self):
        numOuterLoopsToMap = int(self.__parentDirective().numCollapse())
        if LOOP_COLLAPSE_STRATEGY == "grid" or numOuterLoopsToMap == 1:
            numOuterLoopsToMap = min(3,numOuterLoopsToMap)
            result = ["-1"]*numOuterLoopsToMap
            doLoops = find_all(self.body,TTDoLoop)
            for i,loop in enumerate(doLoops):
                if i < numOuterLoopsToMap:
                    result[i] = loop.problemSizeCStr()
            return result
        else: # "collapse"
            result = ""
            doLoops = find_all(self.body,TTDoLoop)
            for loop in reversed(doLoops[0:numOuterLoopsToMap]):
                if len(result):
                    result += "*"
                result += loop.problemSizeCStr()
            if len(result):
                return [result]
            else:
                return ["-1"]
    def asyncNowait(): 
        """value != CLAUSE_NOT_FOUND means True"""
        return self.__parentDirective().asyncNowait() 
    def depend(self): 
        return self.__parentDirective().depend()
    def deviceTypes(self): 
        return self.__parentDirective().deviceTypes()
    def ifCondition(self): 
        return self.__parentDirective().ifCondition()
    def selfCondition(self): 
        return self.__parentDirective().selfCondition
    def deviceptrs(self,scope=[]):
        if self.__parentDirective().allArraysAreOnDevice():
            return self.arrays_in_body(scope)
        else:
            return self.__parentDirective().deviceptrs()
    def createAllocVars(self):
        return self.__parentDirective().createAllocVars()
    def no_createVars(self):
        return self.__parentDirective().no_createVars()
    def presentVars(self):
        return self.__parentDirective().presentVars()
    def deleteReleaseVars(self):
        return self.__parentDirective().deleteReleaseVars()
    def copyMapToFromVars(self):
        return self.__parentDirective().copyMapToFromVars()
    def copyinMapToVars(self):
        return self.__parentDirective().copyinMapToVars()
    def copyoutMapFromVars(self):
        return self.__parentDirective().copyoutMapFromVars()
    def attachVars(self):
        return self.__parentDirective().attachVars()
    def detachVars(self):
        return self.__parentDirective().detachVars()
    def presentByDefault(self): 
        return self.__parentDirective().presentByDefault()
    def gridExpressionFStr(self):
        """ only CUF """
        return self.__firstLoopAnnotation().gridExpressionFStr()
    def blockExpressionFStr(self):
        """ only CUF """
        return self.__firstLoopAnnotation().blockExpressionFStr()
    def gangTeamPrivateVars(self,converter=make_f_str): 
        return self.__parentDirective().gangTeamPrivateVars(converter)
    def gangTeamFirstprivateVars(self,converter=make_f_str): 
        return self.__parentDirective().gangTeamFirstprivateVars(converter)
    def gangTeamReductions(self,converter=make_f_str): 
        if type(self.__firstLoopAnnotation()) is TTCufKernelDo:
            return { "UNKNOWN" : self.reductionCandidates() } # TODO default reduction type should be configurable
        else:
            return self.__firstLoopAnnotation().reductions(converter)
    def stream(self,converter=make_f_str):
        return self.__parentDirective().stream(converter)
    def sharedMem(self,converter=make_f_str):
        return self.__parentDirective().sharedMem(converter)
    def ompFStr(self,f_snippet):
        """
        :note: The string used for parsing was preprocessed. Hence
               we pass the original Fortran snippet here.
        """
        # TODO There is only one loop or loop-like expression
        # in a parallel loop.
        # There might me multiple loops or look-like expressions
        # in a kernels region.
        # kernels directives must be split
        # into multiple clauses.
        # In all cases the begin and end directives must
        # be consumed.
        # TODO find out relevant directives
        # TODO transform string
        # TODO preprocess Fortran colon expressions
        inout_arrays_in_body = self.inout_arrays_in_body()
        arrays_in_body      = self.arrays_in_body()
        reduction         = self.gangTeamReductions()
        depend            = self.depend()

        if type(self.__parentDirective()) is TTCufKernelDo:
            def cufKernelDoRepl(parseResult):
                nonlocal arrays_in_body
                nonlocal inout_arrays_in_body
                nonlocal reduction
                return parseResult.ompFStr(arrays_in_body,inoutarrays_in_body,reduction,depend), True
            
            result,_ = utils.pyparsingutils.replaceFirst(f_snippet,\
                cuf_kernel_do,\
                cufKernelDoRepl)
            return result
        else:
            def accComputeRepl(parseResult):
                nonlocal arrays_in_body
                nonlocal inout_arrays_in_body
                nonlocal reduction
                return parseResult.ompFStr(arrays_in_body,inoutarrays_in_body,depend), True
            parallelRegion = "parallel" 
            def accLoopRepl(parseResult):
                nonlocal arrays_in_body
                nonlocal inout_arrays_in_body
                nonlocal reduction
                nonlocal parallelRegion
                result = parseResult.ompFStr("do",parallelRegion)
                parallelRegion = ""
                return result, True
            def accEndRepl(parseResult):
                nonlocal arrays_in_body
                nonlocal inout_arrays_in_body
                nonlocal reduction
                return parseResult.strip()+"!$omp end target", True
            
            result,_ = utils.pyparsingutils.replaceFirst(f_snippet,\
                    acc_parallel | acc_parallel_loop | acc_kernels | acc_kernels_loop,\
                    accComputeRepl)
            result,_ = utils.pyparsingutils.replaceAll(result,\
                    acc_loop,\
                    accLoopRepl)
            result,_ = utils.pyparsingutils.replaceFirst(result,\
                    Optional(White(),default="") + ( ACC_END_PARALLEL | ACC_END_KERNELS ),
                    accEndRepl)
            result,_ = utils.pyparsingutils.eraseAll(result,\
                    ACC_END_PARALLEL_LOOP | ACC_END_KERNELS_LOOP)
            return result
    def c_str(self):
        """
        This routine generates an HIP kernel body.
        """
        # 0. Clarify types of function calls / tensor access that are not 
        # members of a struct
        __flag_tensors(self,self.scope)
        # TODO look up correct signature of called device functions from index
        # 1.1 Collapsing
        numOuterLoopsToMap = int(self.__parentDirective().numCollapse())
        if LOOP_COLLAPSE_STRATEGY == "grid" and numOuterLoopsToMap <= 3:
            dim=numOuterLoopsToMap
        else: # "collapse" or numOuterLoopsToMap > 3
            dim=1
        tidx = "__gidx{dim}".format(dim=dim)
        # 1. unpack colon (":") expressions 
        for expr in find_all(self.body,TTStatement): 
            if type(expr._statement[0]) is TTAssignment:
                expr._statement[0] = expr._statement[0].convertToDoLoopNestIfNecessary()
        # 2. Identify reduced variables
        for expr in find_all(self.body,TTAssignment):
            for var in find_all_matching(expr,lambda x: isinstance(x,IValue)):
                if type(var._value) in [TTDerivedTypeMember,TTIdentifier]:
                    for op,reducedVariables in self.gangTeamReductions().items():
                        if var.name().lower() in [el.lower() for el in reducedVariables]:
                            var._reductionIndex = tidx
            # TODO identify what operation is performed on the highest level to 
            # identify reduction op
        reductionPreamble = ""
        # 2.1. Add init preamble for reduced variables
        for kind,reducedVariables in self.gangTeamReductions(make_c_str).items():
            for var in reducedVariables: 
                reductionPreamble += "reduce_op_{kind}::init({var}[{tidx}]);\n".format(kind=kind,var=var,tidx=tidx)
        # 3. collapse and transform do-loops
        doLoops = find_all(self.body,TTDoLoop)
        if numOuterLoopsToMap == 1 or (LOOP_COLLAPSE_STRATEGY == "grid" and numOuterLoopsToMap <= 3):
            if numOuterLoopsToMap > 3:
                utils.logging.logWarn("loop collapse strategy grid chosen with nested loops > 3")
            numOuterLoopsToMap = min(3,numOuterLoopsToMap)
            threadIndices = ["x","y","z"]
            for i in range(0,3-numOuterLoopsToMap):
                threadIndices.pop()
            indices    = ""
            conditions = []
            for loop in doLoops:
                if not len(threadIndices):
                    break
                loop.setHipThreadIndex(threadIndices.pop())
                indices   += loop.hipThreadIndexCStr()
                conditions.append(loop.hipThreadBoundCStr()) 
        else: # "collapse" or numOuterLoopsToMap > 3
            indices    = ""
            conditions = []
            denominatorFactors = []
            for loop in reversed(doLoops[0:numOuterLoopsToMap]):
                loop.setHipThreadIndex(tidx)
                # denominator1 = "" 
                # denominator2 = "/" + "(end1 - begin1 + 1)"
                # denominator3 = "/" + "(end1 - begin1 + 1)*(end1 - begin1 + 1)"
                if len(denominatorFactors):
                    indices += loop.collapsedLoopIndexCStr("/("+"*".join(denominatorFactors)+")")
                else:
                    indices += loop.collapsedLoopIndexCStr("")
                denominatorFactors.append(loop.problemSizeCStr())
                conditions.append(loop.hipThreadBoundCStr())
        c_snippet = "{0}{2}if ({1}) {{\n{3}}}".format(\
            indices,"&&".join(conditions),reductionPreamble,make_c_str(self.body))
        return postprocessC_snippet(c_snippet)

class TTProcedureBody(TTContainer):
    def _assign_fields(self,tokens):
        self.body        = tokens[0]
        self.scope       = []
        self.result_name  = ""
    def variables_in_body(self,scope=[]):
        """
        :return: all identifiers of LValue and RValues in the body.
        """
        if len(scope):
            self.scope = scope
        return _intrnl_variables_in_body(self,self.scope)
    def arrays_in_body(self,scope=[]):
        if len(scope):
            self.scope = scope
        return _intrnl_arrays_in_body(self,self.scope)
    def inout_arrays_in_body(self,scope=[]):
        if len(scope):
            self.scope = scope
        return _intrnl_inout_arrays_in_body(self,self.scope)
    def c_str(self):
        """
        :return: body of a procedure as C/C++ code.
        Non-empty result names will be propagated to
        all return statements.
        """
        # 0. Clarify types of function calls / tensor access that are not 
        # members of a struct
        _intrnl_flag_tensors(self,self.scope)

        # 2. Propagate result variable name to return statements
        if len(self.result_name):
            for expr in find_all(self.body,TTReturn):
                 expr._result_name = result_name  
        cBody = make_c_str(self.body)
        
        if len(self.result_name):
             cBody += "\nreturn "+result_name+";"
        return cBody

def formatDirective(directiveLine,maxLineWidth):
    result   = ""
    line     = ""
    tokens   = directiveLine.split(" ")
    sentinel = tokens[0]
    for tk in tokens:
        if len(line+tk) > maxLineWidth-1:
            result += line + "&\n"
            line = sentinel+" "
        line += tk+" "
    result += line.rstrip()
    return result