# SPDX-License-Identifier: MIT                                                
# Copyright (c) 2021 GPUFORT Advanced Micro Devices, Inc. All rights reserved.
# extensions for directive-based programming

class ILoopAnnotation():
    def numCollapse(self):
        return CLAUSE_NOT_FOUND
    def tileSizes(self):
        return [CLAUSE_NOT_FOUND]
    def gridExpressionFStr(self):
        """ only CUF """
        return ""
    def blockExpressionFStr(self):
        """ only CUF """
        return ""
    def numGangsTeamsBlocks(self):
        return [CLAUSE_NOT_FOUND]
    def numThreadsInBlock(self):
        return [CLAUSE_NOT_FOUND]
    def numWorkers(self): 
        """ only ACC """
        return CLAUSE_NOT_FOUND
    def simdlenVectorLength(self):
        return CLAUSE_NOT_FOUND
    def dataIndependentIterations(self):
        return True
    def privateVars(self,converter=make_f_str): 
        """ CUF,ACC: all scalars are private by default """ 
        return []
    def lastprivateVars(self,converter=make_f_str): 
        """ only OMP """
        return []
    def reductions(self,converter=make_f_str): 
        """ CUF: Scalar lvalues are reduced by default """
        return {}
    def sharedVars(self,converter=make_f_str): 
        """ only OMP """
        return []
    def allArraysAreOnDevice(self):
        """ only True for CUF kernel do directive """
        return False

class TTDoLoop(TTContainer):
    def _assign_fields(self,tokens):
        # Assignment, number | variable
        self.annotation, self._begin, self._end, self._step, self.body = tokens
        if self.annotation == None:
            self.annotation = ILoopAnnotation()
        self._threadIndex = None # "z","y","x"
    def setHipThreadIndex(self,name):
        self._threadIndex = name
    def hipThreadIndexCStr(self):
        indexVar = self.loopVar()
        begin    = make_c_str(self._begin._rhs) # array indexing is corrected in index macro
        step     = make_c_str(self._step)
        return "int {var} = {begin} + ({step})*(threadIdx.{idx} + blockIdx.{idx} * blockDim.{idx});\n".format(\
                var=indexVar,begin=begin,idx=self._threadIndex,step=step)
    def collapsedLoopIndexCStr(self,denominator):
        indexVar = self.loopVar()
        tid      = self._threadIndex
        assert not tid is None
        begin    = make_c_str(self._begin._rhs)
        size     = self.problemSizeCStr()
        step     = make_c_str(self._step)
        # int i<n> = begin<n> + step<n>*(i<denominator<n>> % size<n>)
        return "int {var} = {begin} + ({step})*({tid}{denom} % {size});\n".format(\
                var=indexVar,begin=begin,tid=tid,denom=denominator,size=size,step=step)
    def problemSizeCStr(self):
        if self._step == "1":
            return "(1 + (({end}) - ({begin})))".format(\
                begin=make_c_str(self._begin._rhs),end=make_c_str(self._end),step=make_c_str(self._step) )
        else:
            return "(1 + (({end}) - ({begin}))/({step}))".format(\
                begin=make_c_str(self._begin._rhs),end=make_c_str(self._end),step=make_c_str(self._step))
    def hipThreadBoundCStr(self) :
        indexVar = self.loopVar()
        begin    = make_c_str(self._begin._rhs)
        end      = make_c_str(self._end)
        step     = make_c_str(self._step)
        return "loop_cond({0},{1},{2})".format(indexVar, end, step)
    def loopVar(self,converter=make_c_str):
        return converter(self._begin._lhs)
    def c_str(self):
        indexVar    = self.loopVar()
        begin       = make_c_str(self._begin._rhs) # array indexing is corrected in index macro
        end         = make_c_str(self._end)
        step        = make_c_str(self._step)
        bodyContent = flattenBody(self.body) 
        if self._threadIndex == None:
            return "for (int {0}={1}; {0} <= {2}; {0} += {3}) {{\n  {4}\n}}".format(indexVar, begin, end, step, bodyContent)
        else:
            return bodyContent

class IComputeConstruct():
    def numCollapse(self):
        return CLAUSE_NOT_FOUND
    def numDimensions(self):
        return 1
    def gridExpressionFStr(self):
        """ only CUF """
        return None
    def blockExpressionFStr(self):
        """ only CUF """
        return None
    def numGangsTeamsBlocks(self):
        return [CLAUSE_NOT_FOUND]
    def numThreadsInBlock(self):
        return [CLAUSE_NOT_FOUND]
    def gangTeamPrivateVars(self,converter=make_f_str): 
        return []
    def gangTeamFirstprivateVars(self,converter=make_f_str): 
        return []
    def gangTeamReductions(self,converter=make_f_str): 
        """ CUF,ACC: all scalars are private by default """ 
        return {}
    def identifiersInBody(self,converter=make_f_str):
        return []
    def arraysInBody(self,converter=make_f_str):
        return []
    def inoutArraysInBody(self,converter=make_f_str):
        return []
    def localScalars(self):
        return []
    def reductionCandidates(self,scope=[]):
        return []
    def loopVars(self):
        return []
    def problemSize(self):
        return []
    def asyncNowait(): 
        """value != CLAUSE_NOT_FOUND means True"""
        return CLAUSE_NOT_FOUND
    def stream(self,converter=make_f_str):
        return "c_null_ptr"
    def sharedMem(self,converter=make_f_str):
        return "0"
    def useDefaultStream(self):
        return True
    def depend(self): 
        """ only OMP """
        #return { "in":[], "out":[], "inout":[], "inout":[], "mutexinoutset":[], "depobj":[] }
        return {}
    def deviceTypes(self): 
        return "*"
    def ifCondition(self): 
        """ OMP,ACC: accelerate only if condition is satisfied. Empty string means condition is satisfied. """
        return ""
    def selfCondition(self): 
        """ OMP,ACC: run on current CPU / device (and do not offload) """
        return ""
    def deviceptrs(self,scope=[]):
        return []
    def createAllocVars(self):
        return []
    def no_createVars(self):
        """ only ACC"""
        return []
    def presentVars(self):
        """ only ACC"""
        return []
    def deleteReleaseVars(self):
        return []
    def copyMapToFromVars(self):
        return []
    def copyinMapToVars(self):
        return []
    def copyoutMapFromVars(self):
        return []
    def attachVars(self):
        """ only ACC """
        return []
    def detachVars(self):
        """ only ACC """
        return []
    def presentByDefault(self):
        """ only ACC parallel """
        return True
    def c_str(self):
        return ""

class EmptyComputeConstruct(IComputeConstruct):
    """
    Fallback for failed full parse.
    Inteneded to reveal as much information as possible
    """
    def __init__(self,f_snippet):
        self._f_snippet   = f_snippet
        self._scope      = scoper.EMPTY_SCOPE
    def identifiersInBody(self,converter=make_f_str):
        valueType = lvalue | rvalue
        identifierNames = []
        for ident,_,__ in valueType.scanString(self._f_snippet): # includes the identifiers of the function calls
            name      = converter(ident._value)
            nameLower = name.lower()
            definition,foundInIndex = scoper.searchScopeForVariable(self._scope,createIndexSearchTagForVariable(nameLower))
            if (foundInIndex or\
               (not nameLower in KEYWORDS and\
               not nameLower in GPUFORT_CPP_ROUTINES)) and\
               not name in identifierNames: # using set destroys order
                identifierNames.append(name)
        return identifierNames

class TTLoopKernel(TTNode,IComputeConstruct):
    def _assign_fields(self,tokens):
        self._parentDirective, self.body = tokens
        self.scope = scoper.EMPTY_SCOPE
    def __firstLoopAnnotation(self):
        return self.body.annotation
    def __parentDirective(self):
        if self._parentDirective == None:
            return self._firstLoopAnnotation()
        else:
            return self._parentDirective
    def loopVars(self):
        identifierNames = []
        doLoops  = find_all(self.body,TTDoLoop)
        for loop in doLoops:
            identifierNames.append(loop.loopVar(make_f_str))
        return identifierNames
    def __searchValueInBody(self,searchFilter,scope,minRank=-1e20):
        #TODO exclude other annotations as well from this search
        def find_all_matching_exclude_directives_(body,filterExpr=lambda x: True,N=-1):
            """
            Find all nodes in tree of type 'searchedType'.
            """
            result = []
            def descend_(curr):
                if (N > 0 and len(result) > N) or isinstance(curr,ILoopAnnotation):
                    return
                if filterExpr(curr):
                    result.append(curr)
                if isinstance(curr,ParseResults) or\
                   isinstance(curr,list):
                    for el in curr:
                        descend_(el)
                elif isinstance(curr,TTNode):
                    for el in curr.children():
                        descend_(el)
            descend_(body)
            return result
        
        identifierNames = [] 
        for valueType in find_all_matching_exclude_directives_(self.body,searchFilter): # includes the identifiers of the function calls
            name = make_f_str(valueType.name())
            nameLower = name.lower()
            definition,foundInIndex = scoper.searchScopeForVariable(\
              scope,createIndexSearchTagForVariable(nameLower))
            if (foundInIndex or\
                (not nameLower in KEYWORDS and\
                not nameLower in GPUFORT_CPP_ROUTINES)) and\
                definition["rank"] >= minRank and\
                not name in identifierNames: # using set destroys order
                     identifierNames.append(name)
        return identifierNames
    def identifiersInBody(self,scope=[]):
        """
        :return: all identifiers of LValue and RValues in the body.
        """
        if len(scope):
            self.scope = scope
        def searchFilter(node):
            return isinstance(node, IValue) and\
                   type(node._value) in [TTDerivedTypeMember,TTIdentifier,TTFunctionCallOrTensorAccess]
        result = self.__searchValueInBody(searchFilter,self.scope)
        return result
    def arraysInBody(self,scope=[]):
        if len(scope):
            self.scope = scope
        def searchFilter(node):
            return isinstance(node,IValue) and\
                    type(node._value) is TTFunctionCallOrTensorAccess 
        return self.__searchValueInBody(searchFilter,self.scope,1)
    def inoutArraysInBody(self,scope=[]):
        if len(scope):
            self.scope = scope
        def searchFilter(node):
            return type(node) is TTLValue and\
                    type(node._value) is TTFunctionCallOrTensorAccess 
        return self.__searchValueInBody(searchFilter,self.scope,1)
    def __localScalarsAndReductionCandidates(self,scope):
        """
        local variable      - scalar variable that is not read before the assignment (and is no derived type member)
        reductionCandidates - scalar variable that is written but not read anymore 

        NOTE: Always returns Fortran identifiers
        NOTE: The loop variables need to be removed from this result when rendering the corresponding C kernel.
        NOTE: Implementatin assumes that loop condition variables are not written to in loop body. 
        NOTE: When rendering the kernel, it is best to exclude all variables for which an array declaration has been found,
        from the result list. TTCufKernelDo instances do not know of the type of the variables.
        """
        scalarsReadSoFar   = [] # per line, with name of lhs scalar removed from list
        initializedScalars = [] 

        # depth first search
        assignments = find_all_matching(self.body,
                lambda node: type(node) in [TTAssignment,TTComplexAssignment,TTMatrixAssignment])
        for assignment in assignments:   
            # lhs scalars
            lvalue     = assignment._lhs._value
            lvalueName = make_f_str(lvalue)
            if type(lvalue) is TTIdentifier: # could still be a matrix
                definition,found_in_scope = scoper.searchScopeForVariable(\
                  scope,createIndexSearchTagForVariable(lvalueName))
                if not found_in_scope or definition["rank"] == 0 and\
                        not lvalueName.lower() in scalarsReadSoFar:
                            initializedScalars.append(lvalueName) # read and initialized in 
            # rhs scalars
            rhsIdentifiers = find_all(assignment._rhs,TTIdentifier)
            for ttidentifier in rhsIdentifiers:
                nameLower = ttidentifier.f_str().lower()
                definition,found_in_scope = scoper.searchScopeForVariable(scope,\
                  createIndexSearchTagForVariable(nameLower))
                if (not found_in_scope or definition["rank"] == 0) and\
                        nameLower != lvalueName.lower(): # do not include name of lhs if lhs appears in rhs
                            scalarsReadSoFar.append(nameLower)
        # initialized scalars that are not read (except in same statement) are likely reductions
        # initialized scalars that are read again in other statements are likely local variables
        reductionCandidates = [name for name in initializedScalars if name not in scalarsReadSoFar]
        localScalars        = [name for name in initializedScalars if name not in reductionCandidates] # contains loop variables
        loopVars = [var.lower() for var in self.loopVars()]
        for var in list(localScalars):
            if var.lower() in loopVars:
                localScalars.remove(var)
        return localScalars, reductionCandidates
    def localScalars(self,scope=[]):
        if len(scope):
            self.scope = scope
        localScalars,_ = self.__localScalarsAndReductionCandidates(self.scope)
        return localScalars 
    def reductionCandidates(self,scope=[]):
        if len(scope):
            self.scope = scope
        _,reductionCandidates = self.__localScalarsAndReductionCandidates(self.scope)
        return reductionCandidates
    def problemSize(self):
        numOuterLoopsToMap = int(self.__parentDirective().numCollapse())
        if LOOP_COLLAPSE_STRATEGY == "grid" or numOuterLoopsToMap == 1:
            numOuterLoopsToMap = min(3,numOuterLoopsToMap)
            result = ["-1"]*numOuterLoopsToMap
            doLoops = find_all(self.body,TTDoLoop)
            for i,loop in enumerate(doLoops):
                if i < numOuterLoopsToMap:
                    result[i] = loop.problemSizeCStr()
            return result
        else: # "collapse"
            result = ""
            doLoops = find_all(self.body,TTDoLoop)
            for loop in reversed(doLoops[0:numOuterLoopsToMap]):
                if len(result):
                    result += "*"
                result += loop.problemSizeCStr()
            if len(result):
                return [result]
            else:
                return ["-1"]
    def asyncNowait(): 
        """value != CLAUSE_NOT_FOUND means True"""
        return self.__parentDirective().asyncNowait() 
    def depend(self): 
        return self.__parentDirective().depend()
    def deviceTypes(self): 
        return self.__parentDirective().deviceTypes()
    def ifCondition(self): 
        return self.__parentDirective().ifCondition()
    def selfCondition(self): 
        return self.__parentDirective().selfCondition
    def deviceptrs(self,scope=[]):
        if self.__parentDirective().allArraysAreOnDevice():
            return self.arraysInBody(scope)
        else:
            return self.__parentDirective().deviceptrs()
    def createAllocVars(self):
        return self.__parentDirective().createAllocVars()
    def no_createVars(self):
        return self.__parentDirective().no_createVars()
    def presentVars(self):
        return self.__parentDirective().presentVars()
    def deleteReleaseVars(self):
        return self.__parentDirective().deleteReleaseVars()
    def copyMapToFromVars(self):
        return self.__parentDirective().copyMapToFromVars()
    def copyinMapToVars(self):
        return self.__parentDirective().copyinMapToVars()
    def copyoutMapFromVars(self):
        return self.__parentDirective().copyoutMapFromVars()
    def attachVars(self):
        return self.__parentDirective().attachVars()
    def detachVars(self):
        return self.__parentDirective().detachVars()
    def presentByDefault(self): 
        return self.__parentDirective().presentByDefault()
    def gridExpressionFStr(self):
        """ only CUF """
        return self.__firstLoopAnnotation().gridExpressionFStr()
    def blockExpressionFStr(self):
        """ only CUF """
        return self.__firstLoopAnnotation().blockExpressionFStr()
    def gangTeamPrivateVars(self,converter=make_f_str): 
        return self.__parentDirective().gangTeamPrivateVars(converter)
    def gangTeamFirstprivateVars(self,converter=make_f_str): 
        return self.__parentDirective().gangTeamFirstprivateVars(converter)
    def gangTeamReductions(self,converter=make_f_str): 
        if type(self.__firstLoopAnnotation()) is TTCufKernelDo:
            return { "UNKNOWN" : self.reductionCandidates() } # TODO default reduction type should be configurable
        else:
            return self.__firstLoopAnnotation().reductions(converter)
    def stream(self,converter=make_f_str):
        return self.__parentDirective().stream(converter)
    def sharedMem(self,converter=make_f_str):
        return self.__parentDirective().sharedMem(converter)
    def ompFStr(self,f_snippet):
        """
        :note: The string used for parsing was preprocessed. Hence
               we pass the original Fortran snippet here.
        """
        # TODO There is only one loop or loop-like expression
        # in a parallel loop.
        # There might me multiple loops or look-like expressions
        # in a kernels region.
        # kernels directives must be split
        # into multiple clauses.
        # In all cases the begin and end directives must
        # be consumed.
        # TODO find out relevant directives
        # TODO transform string
        # TODO preprocess Fortran colon expressions
        inoutArraysInBody = self.inoutArraysInBody()
        arraysInBody      = self.arraysInBody()
        reduction         = self.gangTeamReductions()
        depend            = self.depend()

        if type(self.__parentDirective()) is TTCufKernelDo:
            def cufKernelDoRepl(parseResult):
                nonlocal arraysInBody
                nonlocal inoutArraysInBody
                nonlocal reduction
                return parseResult.ompFStr(arraysInBody,inoutArraysInBody,reduction,depend), True
            
            result,_ = utils.pyparsingutils.replaceFirst(f_snippet,\
                cuf_kernel_do,\
                cufKernelDoRepl)
            return result
        else:
            def accComputeRepl(parseResult):
                nonlocal arraysInBody
                nonlocal inoutArraysInBody
                nonlocal reduction
                return parseResult.ompFStr(arraysInBody,inoutArraysInBody,depend), True
            parallelRegion = "parallel" 
            def accLoopRepl(parseResult):
                nonlocal arraysInBody
                nonlocal inoutArraysInBody
                nonlocal reduction
                nonlocal parallelRegion
                result = parseResult.ompFStr("do",parallelRegion)
                parallelRegion = ""
                return result, True
            def accEndRepl(parseResult):
                nonlocal arraysInBody
                nonlocal inoutArraysInBody
                nonlocal reduction
                return parseResult.strip()+"!$omp end target", True
            
            result,_ = utils.pyparsingutils.replaceFirst(f_snippet,\
                    acc_parallel | acc_parallel_loop | acc_kernels | acc_kernels_loop,\
                    accComputeRepl)
            result,_ = utils.pyparsingutils.replaceAll(result,\
                    acc_loop,\
                    accLoopRepl)
            result,_ = utils.pyparsingutils.replaceFirst(result,\
                    Optional(White(),default="") + ( ACC_END_PARALLEL | ACC_END_KERNELS ),
                    accEndRepl)
            result,_ = utils.pyparsingutils.eraseAll(result,\
                    ACC_END_PARALLEL_LOOP | ACC_END_KERNELS_LOOP)
            return result
    def c_str(self):
        """
        This routine generates an HIP kernel body.
        """
        # 0. Clarify types of function calls / tensor access that are not 
        # members of a struct
        for expr in find_all(self.body,IValue):
             if type(expr._value) is TTFunctionCallOrTensorAccess:
                 _, discovered = scoper.searchIndexForVariable(self.scope,createIndexSearchTagForVariable(expr.name()))
                 if discovered:
                     expr._value._isTensorAccess = True3
        # TODO look up correct signature of called device functions from index
        # 1.1 Collapsing
        numOuterLoopsToMap = int(self.__parentDirective().numCollapse())
        if LOOP_COLLAPSE_STRATEGY == "grid" and numOuterLoopsToMap <= 3:
            dim=numOuterLoopsToMap
        else: # "collapse" or numOuterLoopsToMap > 3
            dim=1
        tidx = "__gidx{dim}".format(dim=dim)
        # 1. unpack colon (":") expressions 
        for expr in find_all(self.body,TTStatement): 
            if type(expr._statement[0]) is TTAssignment:
                expr._statement[0] = expr._statement[0].convertToDoLoopNestIfNecessary()
        # 2. Identify reduced variables
        for expr in find_all(self.body,TTAssignment):
            for var in find_all_matching(expr,lambda x: isinstance(x,IValue)):
                if type(var._value) in [TTDerivedTypeMember,TTIdentifier]:
                    for op,reducedVariables in self.gangTeamReductions().items():
                        if var.name().lower() in [el.lower() for el in reducedVariables]:
                            var._reductionIndex = tidx
            # TODO identify what operation is performed on the highest level to 
            # identify reduction op
        reductionPreamble = ""
        # 2.1. Add init preamble for reduced variables
        for kind,reducedVariables in self.gangTeamReductions(make_c_str).items():
            for var in reducedVariables: 
                reductionPreamble += "reduce_op_{kind}::init({var}[{tidx}]);\n".format(kind=kind,var=var,tidx=tidx)
        # 3. collapse and transform do-loops
        doLoops = find_all(self.body,TTDoLoop)
        if numOuterLoopsToMap == 1 or (LOOP_COLLAPSE_STRATEGY == "grid" and numOuterLoopsToMap <= 3):
            if numOuterLoopsToMap > 3:
                utils.logging.logWarn("loop collapse strategy grid chosen with nested loops > 3")
            numOuterLoopsToMap = min(3,numOuterLoopsToMap)
            threadIndices = ["x","y","z"]
            for i in range(0,3-numOuterLoopsToMap):
                threadIndices.pop()
            indices    = ""
            conditions = []
            for loop in doLoops:
                if not len(threadIndices):
                    break
                loop.setHipThreadIndex(threadIndices.pop())
                indices   += loop.hipThreadIndexCStr()
                conditions.append(loop.hipThreadBoundCStr()) 
        else: # "collapse" or numOuterLoopsToMap > 3
            indices    = ""
            conditions = []
            denominatorFactors = []
            for loop in reversed(doLoops[0:numOuterLoopsToMap]):
                loop.setHipThreadIndex(tidx)
                # denominator1 = "" 
                # denominator2 = "/" + "(end1 - begin1 + 1)"
                # denominator3 = "/" + "(end1 - begin1 + 1)*(end1 - begin1 + 1)"
                if len(denominatorFactors):
                    indices += loop.collapsedLoopIndexCStr("/("+"*".join(denominatorFactors)+")")
                else:
                    indices += loop.collapsedLoopIndexCStr("")
                denominatorFactors.append(loop.problemSizeCStr())
                conditions.append(loop.hipThreadBoundCStr())
        c_snippet = "{0}{2}if ({1}) {{\n{3}}}".format(\
            indices,"&&".join(conditions),reductionPreamble,make_c_str(self.body))
        return postprocessC_snippet(c_snippet)

def formatDirective(directiveLine,maxLineWidth):
    result   = ""
    line     = ""
    tokens   = directiveLine.split(" ")
    sentinel = tokens[0]
    for tk in tokens:
        if len(line+tk) > maxLineWidth-1:
            result += line + "&\n"
            line = sentinel+" "
        line += tk+" "
    result += line.rstrip()
    return result
